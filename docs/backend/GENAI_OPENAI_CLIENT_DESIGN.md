# GenAI ë„ë©”ì¸ OpenAI í´ë¼ì´ì–¸íŠ¸ ë¦¬íŒ©í† ë§ ì„¤ê³„

**ì‘ì„±ì¼**: 2025-10-15  
**ëª©ì **: OpenAI í´ë¼ì´ì–¸íŠ¸ ì¤‘ì•™í™” + ëª¨ë¸ ì„ íƒ + RAG í†µí•©

---

## ğŸ“‹ Executive Summary

### í˜„ì¬ ë¬¸ì œì 

1. **ì¤‘ë³µ ì´ˆê¸°í™”**: ê° ì„œë¹„ìŠ¤ë§ˆë‹¤ `AsyncOpenAI` ê°œë³„ ìƒì„± (3íšŒ ì¤‘ë³µ)
2. **í•˜ë“œì½”ë”©ëœ ëª¨ë¸**: `gpt-4-turbo-preview`, `gpt-4o` ë“± ì„œë¹„ìŠ¤ë§ˆë‹¤ ë‹¤ë¦„
3. **ë¹„ìš© ìµœì í™” ë¶€ì¬**: ëª¨ë“  ìš”ì²­ì´ ê³ ê°€ ëª¨ë¸ ì‚¬ìš© (gpt-4 ê³„ì—´)
4. **RAG ë¯¸êµ¬í˜„**: ì‚¬ìš©ì ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ í™œìš© ë¶ˆê°€
5. **ëª¨ë¸ ì„ íƒ ë¶ˆê°€**: ì‚¬ìš©ìê°€ ëª©ì ì— ë§ëŠ” ëª¨ë¸ ì„ íƒ ë¶ˆê°€ëŠ¥

### ê°œì„  ë°©ì•ˆ ìš”ì•½

1. âœ… **ì¤‘ì•™í™”ëœ OpenAI Client Manager** ìƒì„±
2. âœ… **ëª¨ë¸ ì¹´íƒˆë¡œê·¸** (ëª©ì ë³„ ìµœì  ëª¨ë¸ + ê°€ê²©)
3. âœ… **ì‚¬ìš©ì ëª¨ë¸ ì„ íƒ API** (ì„œë¹„ìŠ¤ë³„ í—ˆìš© ë²”ìœ„ ì œí•œ)
4. âœ… **RAG Integration** (ë²¡í„° DB + ì‚¬ìš©ì ë°ì´í„° ê²€ìƒ‰)
5. âœ… **í† í° ì‚¬ìš©ëŸ‰ ì¶”ì ** (ë¹„ìš© ëª¨ë‹ˆí„°ë§)

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

### 1. OpenAI Client Manager (ì¤‘ì•™í™”)

**ìœ„ì¹˜**: `backend/app/services/gen_ai/core/openai_client_manager.py`

```python
"""
OpenAI Client Manager

ëª©ì :
- OpenAI í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ê´€ë¦¬
- ëª¨ë¸ ì¹´íƒˆë¡œê·¸ ë° ì„ íƒ ë¡œì§
- í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
- RAG ì»¨í…ìŠ¤íŠ¸ í†µí•©
"""

from enum import Enum
from typing import Optional, Dict, Any, List
from openai import AsyncOpenAI
from pydantic import BaseModel


class ModelTier(str, Enum):
    """ëª¨ë¸ ë“±ê¸‰ (ê°€ê²© ê¸°ì¤€)"""
    MINI = "mini"           # ê°€ì¥ ì €ë ´ (ë‹¨ìˆœ ì‘ì—…)
    STANDARD = "standard"   # ì¤‘ê°„ (ì¼ë°˜ ì‘ì—…)
    ADVANCED = "advanced"   # ê³ ê¸‰ (ë³µì¡í•œ ì‘ì—…)
    PREMIUM = "premium"     # ìµœê³ ê¸‰ (ìµœê³  ì„±ëŠ¥)


class ModelCapability(str, Enum):
    """ëª¨ë¸ ê¸°ëŠ¥"""
    CHAT = "chat"                    # ì¼ë°˜ ëŒ€í™”
    CODE_GENERATION = "code_gen"     # ì½”ë“œ ìƒì„±
    ANALYSIS = "analysis"            # ë°ì´í„° ë¶„ì„
    REASONING = "reasoning"          # ë³µì¡í•œ ì¶”ë¡ 
    VISION = "vision"                # ì´ë¯¸ì§€ ë¶„ì„
    FUNCTION_CALLING = "function"    # Function Calling


class ModelConfig(BaseModel):
    """ëª¨ë¸ ì„¤ì •"""
    model_id: str                    # ëª¨ë¸ ID (ì˜ˆ: "gpt-4o-mini")
    tier: ModelTier                  # ê°€ê²© ë“±ê¸‰
    capabilities: List[ModelCapability]  # ì§€ì› ê¸°ëŠ¥
    input_price_per_1m: float        # ì…ë ¥ 100ë§Œ í† í°ë‹¹ ê°€ê²© (USD)
    output_price_per_1m: float       # ì¶œë ¥ 100ë§Œ í† í°ë‹¹ ê°€ê²© (USD)
    max_tokens: int                  # ìµœëŒ€ í† í° ìˆ˜
    supports_rag: bool               # RAG ì§€ì› ì—¬ë¶€
    description: str                 # ëª¨ë¸ ì„¤ëª…


# ëª¨ë¸ ì¹´íƒˆë¡œê·¸ (2025ë…„ 10ì›” ê¸°ì¤€ OpenAI ê°€ê²©)
MODEL_CATALOG: Dict[str, ModelConfig] = {
    # Mini ë“±ê¸‰ (ì €ë ´, ë‹¨ìˆœ ì‘ì—…)
    "gpt-4o-mini": ModelConfig(
        model_id="gpt-4o-mini",
        tier=ModelTier.MINI,
        capabilities=[
            ModelCapability.CHAT,
            ModelCapability.CODE_GENERATION,
            ModelCapability.ANALYSIS,
            ModelCapability.FUNCTION_CALLING,
        ],
        input_price_per_1m=0.15,   # $0.15 / 1M tokens
        output_price_per_1m=0.60,  # $0.60 / 1M tokens
        max_tokens=16_384,
        supports_rag=True,
        description="ê°€ì¥ ì €ë ´í•œ GPT-4o ê³„ì—´. ë‹¨ìˆœ ëŒ€í™”, ìš”ì•½, ë¶„ë¥˜ì— ìµœì ",
    ),

    # Standard ë“±ê¸‰ (ì¤‘ê°„ ê°€ê²©, ì¼ë°˜ ì‘ì—…)
    "gpt-4o": ModelConfig(
        model_id="gpt-4o",
        tier=ModelTier.STANDARD,
        capabilities=[
            ModelCapability.CHAT,
            ModelCapability.CODE_GENERATION,
            ModelCapability.ANALYSIS,
            ModelCapability.REASONING,
            ModelCapability.VISION,
            ModelCapability.FUNCTION_CALLING,
        ],
        input_price_per_1m=2.50,   # $2.50 / 1M tokens
        output_price_per_1m=10.00, # $10.00 / 1M tokens
        max_tokens=16_384,
        supports_rag=True,
        description="ê· í˜• ì¡íŒ ì„±ëŠ¥. ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ì— ì í•©",
    ),

    # Advanced ë“±ê¸‰ (ê³ ê°€, ë³µì¡í•œ ì‘ì—…)
    "gpt-4-turbo": ModelConfig(
        model_id="gpt-4-turbo",
        tier=ModelTier.ADVANCED,
        capabilities=[
            ModelCapability.CHAT,
            ModelCapability.CODE_GENERATION,
            ModelCapability.ANALYSIS,
            ModelCapability.REASONING,
            ModelCapability.VISION,
            ModelCapability.FUNCTION_CALLING,
        ],
        input_price_per_1m=10.00,  # $10.00 / 1M tokens
        output_price_per_1m=30.00, # $30.00 / 1M tokens
        max_tokens=128_000,
        supports_rag=True,
        description="ê¸´ ì»¨í…ìŠ¤íŠ¸ ì§€ì›. ë³µì¡í•œ ë¶„ì„, ê¸´ ë¬¸ì„œ ì²˜ë¦¬",
    ),

    # Premium ë“±ê¸‰ (ìµœê³ ê°€, ìµœê³  ì„±ëŠ¥)
    "o1-preview": ModelConfig(
        model_id="o1-preview",
        tier=ModelTier.PREMIUM,
        capabilities=[
            ModelCapability.REASONING,
            ModelCapability.CODE_GENERATION,
            ModelCapability.ANALYSIS,
        ],
        input_price_per_1m=15.00,  # $15.00 / 1M tokens
        output_price_per_1m=60.00, # $60.00 / 1M tokens
        max_tokens=128_000,
        supports_rag=True,
        description="ìµœê³  ì¶”ë¡  ëŠ¥ë ¥. ë³µì¡í•œ ìˆ˜í•™, ê³¼í•™ ë¬¸ì œ í•´ê²°",
    ),
}


class ServiceModelPolicy(BaseModel):
    """ì„œë¹„ìŠ¤ë³„ ëª¨ë¸ ì„ íƒ ì •ì±…"""
    service_name: str
    allowed_tiers: List[ModelTier]     # í—ˆìš©ëœ ëª¨ë¸ ë“±ê¸‰
    default_model: str                 # ê¸°ë³¸ ëª¨ë¸
    required_capabilities: List[ModelCapability]  # í•„ìˆ˜ ê¸°ëŠ¥


# ì„œë¹„ìŠ¤ë³„ ëª¨ë¸ ì •ì±…
SERVICE_MODEL_POLICIES: Dict[str, ServiceModelPolicy] = {
    "narrative_report": ServiceModelPolicy(
        service_name="narrative_report",
        allowed_tiers=[ModelTier.MINI, ModelTier.STANDARD],
        default_model="gpt-4o-mini",  # ë¦¬í¬íŠ¸ ìƒì„±ì€ ì €ë ´í•œ ëª¨ë¸ ì¶©ë¶„
        required_capabilities=[
            ModelCapability.CHAT,
            ModelCapability.ANALYSIS,
        ],
    ),
    "strategy_builder": ServiceModelPolicy(
        service_name="strategy_builder",
        allowed_tiers=[ModelTier.STANDARD, ModelTier.ADVANCED, ModelTier.PREMIUM],
        default_model="gpt-4o",  # ì½”ë“œ ìƒì„±ì€ ì¤‘ê°„ ì´ìƒ ëª¨ë¸ í•„ìš”
        required_capabilities=[
            ModelCapability.CODE_GENERATION,
            ModelCapability.REASONING,
            ModelCapability.FUNCTION_CALLING,
        ],
    ),
    "chatops_advanced": ServiceModelPolicy(
        service_name="chatops_advanced",
        allowed_tiers=[ModelTier.MINI, ModelTier.STANDARD],
        default_model="gpt-4o-mini",  # ì¼ë°˜ ëŒ€í™”ëŠ” ì €ë ´í•œ ëª¨ë¸ ì¶©ë¶„
        required_capabilities=[
            ModelCapability.CHAT,
            ModelCapability.FUNCTION_CALLING,
        ],
    ),
    "prompt_governance": ServiceModelPolicy(
        service_name="prompt_governance",
        allowed_tiers=[ModelTier.MINI],
        default_model="gpt-4o-mini",  # ì •ì±… ì²´í¬ëŠ” ê°„ë‹¨, ì €ë ´í•œ ëª¨ë¸ë¡œ ì¶©ë¶„
        required_capabilities=[
            ModelCapability.CHAT,
            ModelCapability.ANALYSIS,
        ],
    ),
}


class OpenAIClientManager:
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ë§¤ë‹ˆì € (ì‹±ê¸€í†¤)"""

    _instance: Optional["OpenAIClientManager"] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if not hasattr(self, "initialized"):
            from app.core.config import settings

            api_key = settings.OPENAI_API_KEY
            if not api_key:
                raise ValueError("OPENAI_API_KEY not set")

            self.client = AsyncOpenAI(api_key=api_key)
            self.usage_tracker: Dict[str, Dict[str, int]] = {}  # {model: {input_tokens, output_tokens}}
            self.initialized = True

    def get_client(self) -> AsyncOpenAI:
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        return self.client

    def get_model_config(self, model_id: str) -> ModelConfig:
        """ëª¨ë¸ ì„¤ì • ì¡°íšŒ"""
        if model_id not in MODEL_CATALOG:
            raise ValueError(f"Unknown model: {model_id}")
        return MODEL_CATALOG[model_id]

    def get_available_models(
        self,
        service_name: str,
        user_preference: Optional[ModelTier] = None,
    ) -> List[ModelConfig]:
        """ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        if service_name not in SERVICE_MODEL_POLICIES:
            raise ValueError(f"Unknown service: {service_name}")

        policy = SERVICE_MODEL_POLICIES[service_name]

        # í—ˆìš©ëœ ë“±ê¸‰ í•„í„°ë§
        available = [
            model
            for model in MODEL_CATALOG.values()
            if model.tier in policy.allowed_tiers
        ]

        # í•„ìˆ˜ ê¸°ëŠ¥ ì²´í¬
        available = [
            model
            for model in available
            if all(cap in model.capabilities for cap in policy.required_capabilities)
        ]

        # ì‚¬ìš©ì ì„ í˜¸ë„ í•„í„°ë§ (ì„ íƒ)
        if user_preference:
            available = [m for m in available if m.tier == user_preference]

        # ê°€ê²©ìˆœ ì •ë ¬ (ì €ë ´í•œ ê²ƒë¶€í„°)
        available.sort(key=lambda m: m.input_price_per_1m)

        return available

    def get_default_model(self, service_name: str) -> str:
        """ì„œë¹„ìŠ¤ ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜"""
        if service_name not in SERVICE_MODEL_POLICIES:
            raise ValueError(f"Unknown service: {service_name}")
        return SERVICE_MODEL_POLICIES[service_name].default_model

    def validate_model_for_service(
        self,
        service_name: str,
        model_id: str,
    ) -> bool:
        """ì„œë¹„ìŠ¤ì—ì„œ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ê²€ì¦"""
        if service_name not in SERVICE_MODEL_POLICIES:
            return False

        policy = SERVICE_MODEL_POLICIES[service_name]

        if model_id not in MODEL_CATALOG:
            return False

        model = MODEL_CATALOG[model_id]

        # ë“±ê¸‰ ì²´í¬
        if model.tier not in policy.allowed_tiers:
            return False

        # í•„ìˆ˜ ê¸°ëŠ¥ ì²´í¬
        if not all(cap in model.capabilities for cap in policy.required_capabilities):
            return False

        return True

    def track_usage(
        self,
        model_id: str,
        input_tokens: int,
        output_tokens: int,
    ):
        """í† í° ì‚¬ìš©ëŸ‰ ì¶”ì """
        if model_id not in self.usage_tracker:
            self.usage_tracker[model_id] = {
                "input_tokens": 0,
                "output_tokens": 0,
                "total_cost_usd": 0.0,
            }

        self.usage_tracker[model_id]["input_tokens"] += input_tokens
        self.usage_tracker[model_id]["output_tokens"] += output_tokens

        # ë¹„ìš© ê³„ì‚°
        model_config = self.get_model_config(model_id)
        input_cost = (input_tokens / 1_000_000) * model_config.input_price_per_1m
        output_cost = (output_tokens / 1_000_000) * model_config.output_price_per_1m

        self.usage_tracker[model_id]["total_cost_usd"] += input_cost + output_cost

    def get_usage_report(self) -> Dict[str, Any]:
        """ì‚¬ìš©ëŸ‰ ë¦¬í¬íŠ¸ ë°˜í™˜"""
        return self.usage_tracker.copy()
```

---

### 2. RAG Integration (ì‚¬ìš©ì ë°ì´í„° ì»¨í…ìŠ¤íŠ¸)

**ìœ„ì¹˜**: `backend/app/services/gen_ai/core/rag_service.py`

```python
"""
RAG Service

ëª©ì :
- ì‚¬ìš©ì ë°ì´í„° ë²¡í„°í™” (Backtest ê²°ê³¼, ì „ëµ ì„±ê³¼ ë“±)
- ìœ ì‚¬ë„ ê²€ìƒ‰ (ë²¡í„° DB: ChromaDB ë˜ëŠ” Pinecone)
- LLM í”„ë¡¬í”„íŠ¸ì— ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
"""

from typing import List, Dict, Any, Optional
from datetime import datetime
from pydantic import BaseModel
import chromadb
from chromadb.config import Settings


class RAGContext(BaseModel):
    """RAG ì»¨í…ìŠ¤íŠ¸ (ê²€ìƒ‰ ê²°ê³¼)"""
    document_id: str
    content: str
    metadata: Dict[str, Any]
    similarity_score: float


class RAGService:
    """RAG ì„œë¹„ìŠ¤ (Retrieval-Augmented Generation)"""

    def __init__(self):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        # ë¡œì»¬ ChromaDB (ê°œë°œ)
        self.client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory="./data/chromadb"
        ))

        # ì»¬ë ‰ì…˜ ìƒì„±/ì¡°íšŒ
        self.backtests_collection = self.client.get_or_create_collection(
            name="user_backtests",
            metadata={"description": "ì‚¬ìš©ì ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼"}
        )

        self.strategies_collection = self.client.get_or_create_collection(
            name="user_strategies",
            metadata={"description": "ì‚¬ìš©ì ì „ëµ ì½”ë“œ ë° ì„±ê³¼"}
        )

    async def index_backtest_result(
        self,
        backtest_id: str,
        user_id: str,
        result_summary: Dict[str, Any],
    ):
        """ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë²¡í„° DBì— ì¸ë±ì‹±"""

        # ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        text_content = self._format_backtest_as_text(result_summary)

        # ë©”íƒ€ë°ì´í„°
        metadata = {
            "backtest_id": backtest_id,
            "user_id": user_id,
            "strategy_name": result_summary.get("strategy_name", "Unknown"),
            "total_return": result_summary.get("total_return", 0.0),
            "sharpe_ratio": result_summary.get("sharpe_ratio", 0.0),
            "created_at": datetime.now().isoformat(),
        }

        # ChromaDBì— ì¶”ê°€
        self.backtests_collection.add(
            documents=[text_content],
            metadatas=[metadata],
            ids=[backtest_id],
        )

    async def search_similar_backtests(
        self,
        user_id: str,
        query: str,
        top_k: int = 5,
    ) -> List[RAGContext]:
        """ì‚¬ìš©ìì˜ ìœ ì‚¬ ë°±í…ŒìŠ¤íŠ¸ ê²€ìƒ‰"""

        # ChromaDB ì¿¼ë¦¬
        results = self.backtests_collection.query(
            query_texts=[query],
            n_results=top_k,
            where={"user_id": user_id},  # ì‚¬ìš©ì í•„í„°ë§
        )

        # RAGContextë¡œ ë³€í™˜
        contexts: List[RAGContext] = []
        for i in range(len(results["ids"][0])):
            contexts.append(
                RAGContext(
                    document_id=results["ids"][0][i],
                    content=results["documents"][0][i],
                    metadata=results["metadatas"][0][i],
                    similarity_score=results["distances"][0][i],
                )
            )

        return contexts

    async def build_rag_prompt(
        self,
        user_query: str,
        user_id: str,
        context_type: str = "backtests",
        top_k: int = 3,
    ) -> str:
        """RAG í”„ë¡¬í”„íŠ¸ ìƒì„± (ì‚¬ìš©ì ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ í¬í•¨)"""

        # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        if context_type == "backtests":
            contexts = await self.search_similar_backtests(user_id, user_query, top_k)
        else:
            contexts = []  # ë‹¤ë¥¸ íƒ€ì… ì¶”ê°€ ê°€ëŠ¥

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if not contexts:
            return user_query

        context_text = "\n\n".join([
            f"[ì°¸ê³  {i+1}] {ctx.content}"
            for i, ctx in enumerate(contexts)
        ])

        prompt = f"""ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ê³¼ê±° ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ì…ë‹ˆë‹¤:

{context_text}

ì‚¬ìš©ì ìš”ì²­: {user_query}

ìœ„ ê³¼ê±° ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”."""

        return prompt

    def _format_backtest_as_text(self, result: Dict[str, Any]) -> str:
        """ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        return f"""
ì „ëµëª…: {result.get('strategy_name', 'Unknown')}
ê¸°ê°„: {result.get('start_date', 'N/A')} ~ {result.get('end_date', 'N/A')}
ì´ ìˆ˜ìµë¥ : {result.get('total_return', 0.0):.2%}
ìƒ¤í”„ ë¹„ìœ¨: {result.get('sharpe_ratio', 0.0):.2f}
ìµœëŒ€ ë‚™í­: {result.get('max_drawdown', 0.0):.2%}
ì´ ê±°ë˜ íšŸìˆ˜: {result.get('total_trades', 0)}
ìŠ¹ë¥ : {result.get('win_rate', 0.0):.2%}
"""
```

---

### 3. ì„œë¹„ìŠ¤ë³„ ì ìš© ì˜ˆì‹œ

**3.1 StrategyBuilderService (RAG + ëª¨ë¸ ì„ íƒ)**

```python
# backend/app/services/gen_ai/applications/strategy_builder_service.py

from app.services.gen_ai.core.openai_client_manager import OpenAIClientManager
from app.services.gen_ai.core.rag_service import RAGService

class StrategyBuilderService:
    def __init__(
        self,
        strategy_service,
        backtest_service,
    ):
        self.strategy_service = strategy_service
        self.backtest_service = backtest_service

        # ì¤‘ì•™í™”ëœ OpenAI í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        self.openai_manager = OpenAIClientManager()
        self.client = self.openai_manager.get_client()

        # RAG ì„œë¹„ìŠ¤
        self.rag_service = RAGService()

        # ê¸°ë³¸ ëª¨ë¸ (ì‚¬ìš©ì ì„ íƒ ê°€ëŠ¥)
        self.default_model = self.openai_manager.get_default_model("strategy_builder")

    async def generate_strategy_with_rag(
        self,
        user_id: str,
        user_request: str,
        model_preference: Optional[str] = None,  # ì‚¬ìš©ì ëª¨ë¸ ì„ íƒ
    ) -> str:
        """RAG ê¸°ë°˜ ì „ëµ ìƒì„± (ì‚¬ìš©ì ê³¼ê±° ë°±í…ŒìŠ¤íŠ¸ ì°¸ê³ )"""

        # ëª¨ë¸ ì„ íƒ ë° ê²€ì¦
        model_id = model_preference or self.default_model

        if not self.openai_manager.validate_model_for_service("strategy_builder", model_id):
            raise ValueError(f"Model {model_id} not allowed for strategy_builder")

        # RAG í”„ë¡¬í”„íŠ¸ ìƒì„± (ì‚¬ìš©ì ê³¼ê±° ë°±í…ŒìŠ¤íŠ¸ ê²€ìƒ‰)
        rag_prompt = await self.rag_service.build_rag_prompt(
            user_query=user_request,
            user_id=user_id,
            context_type="backtests",
            top_k=3,
        )

        # LLM í˜¸ì¶œ
        response = await self.client.chat.completions.create(
            model=model_id,
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ í€€íŠ¸ íŠ¸ë ˆì´ë”© ì „ëµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ê³¼ê±° ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ìµœì ì˜ ì „ëµ ì½”ë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.",
                },
                {
                    "role": "user",
                    "content": rag_prompt,
                },
            ],
        )

        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
        usage = response.usage
        self.openai_manager.track_usage(
            model_id=model_id,
            input_tokens=usage.prompt_tokens,
            output_tokens=usage.completion_tokens,
        )

        return response.choices[0].message.content
```

**3.2 API ì—”ë“œí¬ì¸íŠ¸ (ëª¨ë¸ ì„ íƒ)**

```python
# backend/app/api/routes/gen_ai/strategy_builder.py

from typing import Optional
from fastapi import APIRouter, Depends
from pydantic import BaseModel

from app.services.gen_ai.core.openai_client_manager import (
    OpenAIClientManager,
    ModelTier,
    ModelConfig,
)
from app.services.gen_ai.applications.strategy_builder_service import (
    StrategyBuilderService,
)

router = APIRouter()


class ModelSelectionRequest(BaseModel):
    """ëª¨ë¸ ì„ íƒ ìš”ì²­"""
    preferred_tier: Optional[ModelTier] = None  # ì‚¬ìš©ì ì„ í˜¸ ë“±ê¸‰


class StrategyGenerateRequest(BaseModel):
    """ì „ëµ ìƒì„± ìš”ì²­"""
    user_request: str
    model_id: Optional[str] = None  # ì‚¬ìš©ì ëª¨ë¸ ì„ íƒ (ì„ íƒ)


@router.get("/models")
async def get_available_models(
    tier: Optional[ModelTier] = None,
) -> List[ModelConfig]:
    """ì „ëµ ë¹Œë”ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
    manager = OpenAIClientManager()
    models = manager.get_available_models(
        service_name="strategy_builder",
        user_preference=tier,
    )
    return models


@router.post("/generate")
async def generate_strategy_with_model(
    request: StrategyGenerateRequest,
    service: StrategyBuilderService = Depends(),
    user_id: str = Depends(get_current_user_id),
) -> Dict[str, Any]:
    """RAG ê¸°ë°˜ ì „ëµ ìƒì„± (ëª¨ë¸ ì„ íƒ ê°€ëŠ¥)"""

    strategy_code = await service.generate_strategy_with_rag(
        user_id=user_id,
        user_request=request.user_request,
        model_preference=request.model_id,
    )

    return {
        "strategy_code": strategy_code,
        "model_used": request.model_id or service.default_model,
    }
```

---

## ğŸ“Š ì ìš© ë²”ìœ„ ë° ìš°ì„ ìˆœìœ„

### Phase 1: ê¸°ë³¸ ì¸í”„ë¼ (1ì£¼)

1. âœ… **OpenAIClientManager êµ¬í˜„** (2ì¼)
   - ëª¨ë¸ ì¹´íƒˆë¡œê·¸
   - ì„œë¹„ìŠ¤ë³„ ì •ì±…
   - í† í° ì¶”ì 
2. âœ… **ì„œë¹„ìŠ¤ ë¦¬íŒ©í† ë§** (3ì¼)
   - StrategyBuilderService
   - NarrativeReportService
   - ChatOpsAdvancedService
   - PromptGovernanceService
3. âœ… **API ì—”ë“œí¬ì¸íŠ¸** (2ì¼)
   - ëª¨ë¸ ì„ íƒ API
   - ì‚¬ìš©ëŸ‰ ì¡°íšŒ API

### Phase 2: RAG í†µí•© (1ì£¼)

1. âœ… **RAGService êµ¬í˜„** (2ì¼)
   - ChromaDB ì„¤ì •
   - ë°±í…ŒìŠ¤íŠ¸ ì¸ë±ì‹±
   - ìœ ì‚¬ë„ ê²€ìƒ‰
2. âœ… **ì„œë¹„ìŠ¤ í†µí•©** (3ì¼)
   - StrategyBuilderService RAG ì ìš©
   - ChatOpsAdvancedService RAG ì ìš©
   - ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ ì‹œ ìë™ ì¸ë±ì‹±
3. âœ… **í…ŒìŠ¤íŠ¸ ë° ê²€ì¦** (2ì¼)
   - RAG í’ˆì§ˆ í…ŒìŠ¤íŠ¸
   - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### Phase 3: ê³ ê¸‰ ê¸°ëŠ¥ (ì„ íƒì )

1. â¸ï¸ **Function Calling í†µí•©**
   - ì‹¤ì‹œê°„ ë°ì´í„° ì¡°íšŒ
   - ë°±í…ŒìŠ¤íŠ¸ ìë™ ì‹¤í–‰
2. â¸ï¸ **ë¹„ìš© ìµœì í™”**
   - ìºì‹± ë ˆì´ì–´
   - í”„ë¡¬í”„íŠ¸ ì••ì¶•
3. â¸ï¸ **ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**
   - ëª¨ë¸ë³„ ì‚¬ìš©ëŸ‰
   - ë¹„ìš© ì¶”ì 
   - ì„±ëŠ¥ ì§€í‘œ

---

## ğŸ’° ë¹„ìš© ìµœì í™” ì „ëµ

### 1. ëª¨ë¸ë³„ ì‚¬ìš© ì‚¬ë¡€

| ì„œë¹„ìŠ¤           | ì‘ì—… ìœ í˜•            | ê¶Œì¥ ëª¨ë¸   | ì›”ê°„ ì˜ˆìƒ ë¹„ìš©\* |
| ---------------- | -------------------- | ----------- | ---------------- |
| NarrativeReport  | ë°±í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± | gpt-4o-mini | $5-10            |
| StrategyBuilder  | ì „ëµ ì½”ë“œ ìƒì„±       | gpt-4o      | $20-50           |
| ChatOpsAdvanced  | ì¼ë°˜ ëŒ€í™”            | gpt-4o-mini | $2-5             |
| PromptGovernance | ì •ì±… ì²´í¬            | gpt-4o-mini | $1-2             |

\*_1000ëª… ì‚¬ìš©ì ê¸°ì¤€, ì›” 10,000 ìš”ì²­ ê°€ì •_

### 2. ë¹„ìš© ì ˆê° íŒ

1. **ì‘ì—…ë³„ ëª¨ë¸ ë¶„ë¦¬**: ë‹¨ìˆœ ìš”ì•½ â†’ mini, ë³µì¡í•œ ì½”ë“œ â†’ advanced
2. **ìºì‹±**: ë™ì¼ ìš”ì²­ â†’ ì¬ì‚¬ìš© (50% ë¹„ìš© ì ˆê°)
3. **í”„ë¡¬í”„íŠ¸ ìµœì í™”**: ë¶ˆí•„ìš”í•œ ì˜ˆì‹œ ì œê±° â†’ í† í° 30% ì ˆê°
4. **RAG í™œìš©**: ì „ì²´ ë°ì´í„° ì£¼ì… ëŒ€ì‹  ê´€ë ¨ ë°ì´í„°ë§Œ â†’ 70% í† í° ì ˆê°

---

## ğŸ” ë³´ì•ˆ ë° ê±°ë²„ë„ŒìŠ¤

### 1. API í‚¤ ê´€ë¦¬

- âœ… í™˜ê²½ë³€ìˆ˜ (.env)
- âœ… ì„œë¹„ìŠ¤ë³„ rate limiting
- âœ… ì‚¬ìš©ìë³„ quota

### 2. í”„ë¡¬í”„íŠ¸ ê±°ë²„ë„ŒìŠ¤

- âœ… PromptGovernanceService í™œìš©
- âœ… ê¸ˆì§€ì–´ í•„í„°ë§
- âœ… PII ë°ì´í„° ë§ˆìŠ¤í‚¹

### 3. ëª¨ë‹ˆí„°ë§

- âœ… í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
- âœ… ë¹„ìš© ì•Œë¦¼ (ì„ê³„ê°’ ì´ˆê³¼ ì‹œ)
- âœ… ëª¨ë¸ë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­

---

## ğŸ“ ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê¸°ì¡´ ì½”ë“œ ì œê±°

- [ ] âŒ `narrative_report_service.py` - `self.client = AsyncOpenAI()` ì‚­ì œ
- [ ] âŒ `strategy_builder_service.py` - `self.client = AsyncOpenAI()` ì‚­ì œ
- [ ] âŒ `chatops_advanced_service.py` - `self.client = AsyncOpenAI()` ì‚­ì œ

### ìƒˆ ì½”ë“œ ì¶”ê°€

- [ ] âœ… `gen_ai/core/openai_client_manager.py` ìƒì„±
- [ ] âœ… `gen_ai/core/rag_service.py` ìƒì„±
- [ ] âœ… ê° ì„œë¹„ìŠ¤ì— `OpenAIClientManager` ì£¼ì…
- [ ] âœ… API ë¼ìš°í„°ì— ëª¨ë¸ ì„ íƒ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

### í™˜ê²½ ë³€ìˆ˜

- [ ] âœ… `OPENAI_API_KEY` í™•ì¸
- [ ] âœ… `CHROMADB_PATH` ì¶”ê°€ (ì„ íƒ)

### í…ŒìŠ¤íŠ¸

- [ ] âœ… ëª¨ë¸ ì¹´íƒˆë¡œê·¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- [ ] âœ… RAG ê²€ìƒ‰ í†µí•© í…ŒìŠ¤íŠ¸
- [ ] âœ… ì„œë¹„ìŠ¤ë³„ E2E í…ŒìŠ¤íŠ¸

---

## ğŸ¯ ì¶”ê°€ ì•„ì´ë””ì–´

### 1. ë©€í‹°ëª¨ë‹¬ ì§€ì› (Vision API)

- ì°¨íŠ¸ ì´ë¯¸ì§€ ë¶„ì„ (ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì°¨íŠ¸)
- ê¸°ìˆ ì  ë¶„ì„ íŒ¨í„´ ì¸ì‹

### 2. Fine-tuning (ì„ íƒì )

- ì‚¬ìš©ì ì „ëµ ìŠ¤íƒ€ì¼ í•™ìŠµ
- ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ (í€€íŠ¸ íŠ¸ë ˆì´ë”©)

### 3. Streaming ì‘ë‹µ

- ê¸´ ë¦¬í¬íŠ¸ ìƒì„± ì‹œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
- ì‚¬ìš©ì ê²½í—˜ ê°œì„ 

### 4. A/B Testing

- ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ (í’ˆì§ˆ vs ë¹„ìš©)
- ìµœì  ëª¨ë¸ ìë™ ì„ íƒ

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [OpenAI Pricing (2025)](https://openai.com/pricing)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)

---

**ë‹¤ìŒ ë‹¨ê³„**: Phase 1 êµ¬í˜„ ì‹œì‘ (OpenAIClientManager)
